import streamlit as st
import re
import textwrap
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound
from urllib.parse import urlparse, parse_qs

def extract_video_id(url):
    parsed_url = urlparse(url)
    if "youtube.com" in parsed_url.netloc:
        query = parse_qs(parsed_url.query)
        return query.get("v", [None])[0]
    elif "youtu.be" in parsed_url.netloc:
        return parsed_url.path.strip("/")
    return None

def get_transcript(video_id):
    """ìœ íŠœë¸Œ ìë§‰ ê°€ì ¸ì˜¤ê¸° (í•œêµ­ì–´ ìš°ì„ , ê·¸ë‹¤ìŒ ì˜ì–´)"""
    try:
        transcripts = YouTubeTranscriptApi.list_transcripts(video_id)
        print("[ë””ë²„ê·¸] ì‚¬ìš© ê°€ëŠ¥í•œ ìë§‰ ëª©ë¡:")
        for t in transcripts:
            print(f" - {t.language_code} | {t.language} | {'ìë™ ìƒì„±' if t.is_generated else 'ì‚¬ìš©ì ì œê³µ'}")
        
        transcript = transcripts.find_transcript(['ko', 'en'])
        print(f"[ë””ë²„ê·¸] ì„ íƒëœ ìë§‰ ì–¸ì–´: {transcript.language_code}")
        text = '\n'.join([entry.text for entry in transcript.fetch()])
        return text
    except (TranscriptsDisabled, NoTranscriptFound) as e:
        print(f"[ë””ë²„ê·¸] ìë§‰ ì—†ìŒ ì˜¤ë¥˜: {e}")
        return None

def structure_transcript(raw_text):
    """í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , ìš”ì•½ ì œëª©ê³¼ í‚¤ì›Œë“œ ê¸°ë°˜ í—¤ë“œë¼ì¸ ìƒì„±"""
    import nltk
    nltk.download('punkt', quiet=True)
    from nltk.tokenize import sent_tokenize
    from sklearn.feature_extraction.text import TfidfVectorizer
    import numpy as np

    # ë¬¸ì¥ ë‚˜ëˆ„ê¸°
    sentences = sent_tokenize(raw_text)
    chunk_size = 4
    chunks = [sentences[i:i + chunk_size] for i in range(0, len(sentences), chunk_size)]

    structured = ""
    for i, chunk in enumerate(chunks, 1):
        paragraph = ' '.join(chunk)

        # í‚¤ì›Œë“œ ì¶”ì¶œìš© TF-IDF
        try:
            vectorizer = TfidfVectorizer(stop_words='english')
            X = vectorizer.fit_transform([paragraph])
            indices = np.argsort(X.toarray()[0])[::-1]
            features = vectorizer.get_feature_names_out()
            top_keywords = [features[j] for j in indices[:3] if j < len(features)]
            keyword_str = ", ".join(top_keywords)
        except:
            keyword_str = "ë‚´ìš© ìš”ì•½"

        headline = f"ğŸ”¹ ì„¹ì…˜ {i} - í•µì‹¬ì–´: {keyword_str}"
        structured += f"{headline}\n{paragraph.strip()}\n\n"

    return structured.strip()

def to_formal_writing(text):
    """êµ¬ì–´ì²´ë¥¼ ë¬¸ì„œì²´ë¡œ ë³€í™˜ (ì˜ˆì‹œ ìˆ˜ì¤€ ë³€í™˜)"""
    conversions = {
        "ê·¸ë‹ˆê¹Œ": "ë”°ë¼ì„œ",
        "ê·¼ë°": "í•˜ì§€ë§Œ",
        "ê·¸ë˜ì„œ": "ê·¸ëŸ¬ë¯€ë¡œ",
        "ì–´ì¨Œë“ ": "ê²°ë¡ ì ìœ¼ë¡œ",
        "ë­”ê°€": "ì–´ë–¤",
        "ì§„ì§œ": "ì •ë§",
        "ë˜ê²Œ": "ë§¤ìš°",
        "ì•½ê°„": "ì¡°ê¸ˆ",
        "ì¢€": "ì¡°ê¸ˆ",
        "ì´ê±°": "ì´ê²ƒì€",
        "ì €ê±°": "ì €ê²ƒì€"
    }
    for informal, formal in conversions.items():
        text = text.replace(informal, formal)
    return text

def main():
    st.title("ğŸ¬ ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œê¸°")
    url = st.text_input("ìœ íŠœë¸Œ URLì„ ì…ë ¥í•˜ì„¸ìš”")

    if url:
        video_id = extract_video_id(url)
        if not video_id:
            st.error("ìœ íš¨í•œ ìœ íŠœë¸Œ URLì´ ì•„ë‹™ë‹ˆë‹¤.")
            return

        transcript = get_transcript(video_id)
        if transcript:
            structured = structure_transcript(transcript)
            formal = to_formal_writing(structured)

            st.text_area("ğŸ“œ ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸", transcript, height=200)
            st.text_area("ğŸ§¾ êµ¬ì¡°í™”ëœ ìë§‰", structured, height=300)
            st.text_area("ğŸ“ ë¬¸ì„œí™”ëœ ìë§‰", formal, height=300)
            
        else:
            st.warning("âŒ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()